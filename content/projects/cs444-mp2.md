+++
authors = ["Ziheng Chen"]
title = "ML: Multi-Layer Neural Networks"
date = "2023-03-06"
tags = [
    "python", "numpy", "machine-learning", "neural-networks", "computer-vision",
    "matplotlib"
]
aliases = ["cs444-mp2"]
+++

![Reconstruction](/images/projects/cs444-mp2.png#center)

[Source (GitHub)](https://github.com/zihengjackchen/CS444-Deep-Learning/tree/main/assignment2%20-%20Multi-Layer%20Neural%20Networks)

[Results](https://github.com/zihengjackchen/CS444-Deep-Learning/blob/main/assignment2%20-%20Multi-Layer%20Neural%20Networks/zihengc2_yutongz7_mp2_report.pdf)

#### Contributions
- Implemented multi-layer neural networks for image reconstruction
- Applied backpropagation to memorize images based on 2-dimensional (x, y) coordinates
- Explored various input feature mapping techniques following recommendations from a relevant paper
- Developed forward and backward passes for the network
- Trained a four-layer network using both Stochastic Gradient Descent (SGD) and Adam optimizers
- Ensured network weights were updated to minimize Mean Square Error (MSE) loss between original and reconstructed images
- Managed hyperparameters, including hidden layer size, learning rate, and activation function choices.
- Documented project details and findings in the 'neural_network.ipnyb' notebook

#### Project Overview
In this project, my objective is to implement multi-layer neural networks and utilize backpropagation to reconstruct an image. The figure below illustrates the process, where the inputs to the network are 2-dimensional (x, y) coordinates, and the outputs are 3-dimensional RGB values. Essentially, the network aims to recreate an image based on these pixel coordinate inputs.

My journey begins with training a model using the (x, y) coordinates directly. Subsequently, I'll explore different forms of input feature mapping, following recommendations outlined in a relevant paper. The overarching goal is to improve the final image reconstruction.

The main tasks involve creating both forward and backward passes, training a four-layer network using both Stochastic Gradient Descent (SGD) and Adam optimizers. The focus is on updating the network's weights to minimize the Mean Square Error (MSE) loss between the original and reconstructed images. Fortunately, I have access to specific hyperparameters, such as hidden layer size, learning rate, and activation function choices, detailed in the 'neural_network.ipnyb' notebook.




[Assignment Page](https://slazebni.cs.illinois.edu/spring23/assignment2.html)
